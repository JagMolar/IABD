{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea para BDA02 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "drwxrwxrwx   - root supergroup          0 2023-01-10 10:03 /tmp\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-01-10 10:03 /user\r\n",
      "drwxrwxrwt   - root root                0 2023-01-10 10:48 /yarn\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app  boot     dev  hdfs  lib\tlib64\tmedia  opt   root  sbin  sys  usr\r\n",
      "bin  dataset  etc  home  lib32\tlibx32\tmnt    proc  run   srv\t tmp  var\r\n"
     ]
    }
   ],
   "source": [
    "! ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de la nota media con Scripts de Bash ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"pedro 6 7\\nluis 0 4\\nana 7\\npedro 8 1 3\\nana 5 6 7\\nana 10\\nluis 3\" > notas.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pedro 6 7\r\n",
      "luis 0 4\r\n",
      "ana 7\r\n",
      "pedro 8 1 3\r\n",
      "ana 5 6 7\r\n",
      "ana 10\r\n",
      "luis 3\r\n"
     ]
    }
   ],
   "source": [
    "! cat notas.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# Leemos línea a línea de la entrada estándar\n",
    "while read line; do\n",
    "    \n",
    "    # Extraemos el nombre de la línea\n",
    "    name=${line%% *}\n",
    "    \n",
    "    # Procesamos nota a nota\n",
    "    for mark in ${line#* }; do\n",
    "                  \n",
    "        # para cada nota emitimos nombre,nota\n",
    "        echo -e \"$name,$mark\"\n",
    "    done    \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x mapper.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana,10\r\n",
      "ana,5\r\n",
      "ana,6\r\n",
      "ana,7\r\n",
      "ana,7\r\n",
      "luis,0\r\n",
      "luis,3\r\n",
      "luis,4\r\n",
      "pedro,1\r\n",
      "pedro,3\r\n",
      "pedro,6\r\n",
      "pedro,7\r\n",
      "pedro,8\r\n"
     ]
    }
   ],
   "source": [
    "! cat notas.txt | ./mapper.sh |sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.sh\n",
    "#!/bin/bash\n",
    "prev_name=\n",
    "acc=0\n",
    "n_marks=0\n",
    "\n",
    "# Leemos línea a línea\n",
    "while read line; do\n",
    "    # Extraemos el nombre y la nota\n",
    "    name=${line%,*}\n",
    "    mark=${line#*,}\n",
    "    \n",
    "    # Si el nombre es igual al de la anterior línea o es la primera iteración, acumulamos suma de notas y el nº de notas\n",
    "    if [ -z \"$prev_name\" -o \"$prev_name\" == \"$name\" ]; then                \n",
    "        let n_marks++\n",
    "        acc=$(($acc + $mark))\n",
    "    \n",
    "    # Cuando el nombre sea diferente, emitimos el nombre anterior,la nota media anterior\n",
    "    else\n",
    "        echo $prev_name,$(($acc/n_marks))\n",
    "        acc=$mark\n",
    "        n_marks=1\n",
    "    fi\n",
    "    prev_name=$name\n",
    "done\n",
    "           \n",
    "# Emitimos el nombre y la nota media del último nombre\n",
    "echo $prev_name,$(($acc/n_marks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x reducer.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana,7\r\n",
      "luis,2\r\n",
      "pedro,5\r\n"
     ]
    }
   ],
   "source": [
    "! cat notas.txt | ./mapper.sh |sort | ./reducer.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/notas.txt\r\n",
      "Deleted /user/root/output\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -f -r /user/root/notas.txt /user/root/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -copyFromLocal notas.txt /user/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup         61 2023-01-10 13:16 /user/root/notas.txt\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-01-10 11:11 /user/root/tmp\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pedro 6 7\r\n",
      "luis 0 4\r\n",
      "ana 7\r\n",
      "pedro 8 1 3\r\n",
      "ana 5 6 7\r\n",
      "ana 10\r\n",
      "luis 3\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /user/root/notas.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar] /tmp/streamjob8879329233620602065.jar tmpDir=null\n",
      "2023-01-10 13:16:14,012 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "2023-01-10 13:16:14,183 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "2023-01-10 13:16:14,412 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1673341397441_0010\n",
      "2023-01-10 13:16:14,836 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2023-01-10 13:16:14,927 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2023-01-10 13:16:14,956 INFO Configuration.deprecation: mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2023-01-10 13:16:15,068 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1673341397441_0010\n",
      "2023-01-10 13:16:15,068 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-01-10 13:16:15,239 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-10 13:16:15,239 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-10 13:16:15,307 INFO impl.YarnClientImpl: Submitted application application_1673341397441_0010\n",
      "2023-01-10 13:16:15,343 INFO mapreduce.Job: The url to track the job: http://yarnmaster:8088/proxy/application_1673341397441_0010/\n",
      "2023-01-10 13:16:15,345 INFO mapreduce.Job: Running job: job_1673341397441_0010\n",
      "2023-01-10 13:16:22,523 INFO mapreduce.Job: Job job_1673341397441_0010 running in uber mode : false\n",
      "2023-01-10 13:16:22,526 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-01-10 13:16:35,782 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-01-10 13:16:39,825 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-01-10 13:16:40,849 INFO mapreduce.Job: Job job_1673341397441_0010 completed successfully\n",
      "2023-01-10 13:16:40,978 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=137\n",
      "\t\tFILE: Number of bytes written=832545\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=276\n",
      "\t\tHDFS: Number of bytes written=24\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18538\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2703\n",
      "\t\tTotal time spent by all map tasks (ms)=18538\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2703\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=18538\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2703\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18982912\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2767872\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=13\n",
      "\t\tMap output bytes=105\n",
      "\t\tMap output materialized bytes=143\n",
      "\t\tInput split bytes=184\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce shuffle bytes=143\n",
      "\t\tReduce input records=13\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=26\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=301\n",
      "\t\tCPU time spent (ms)=1550\n",
      "\t\tPhysical memory (bytes) snapshot=770445312\n",
      "\t\tVirtual memory (bytes) snapshot=7631757312\n",
      "\t\tTotal committed heap usage (bytes)=617611264\n",
      "\t\tPeak Map Physical memory (bytes)=290394112\n",
      "\t\tPeak Map Virtual memory (bytes)=2545790976\n",
      "\t\tPeak Reduce Physical memory (bytes)=196190208\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2545078272\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=24\n",
      "2023-01-10 13:16:40,978 INFO streaming.StreamJob: Output directory: /user/root/output\n"
     ]
    }
   ],
   "source": [
    "! mapred streaming \\\n",
    "    -D mapred.textoutputformat.separator=\",\" \\\n",
    "    -files /media/notebooks/mapper.sh,/media/notebooks/reducer.sh \\\n",
    "    -input /user/root/notas.txt \\\n",
    "    -output /user/root/output \\\n",
    "    -mapper mapper.sh \\\n",
    "    -reducer reducer.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2023-01-10 13:16 /user/root/output/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup         24 2023-01-10 13:16 /user/root/output/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/root/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana,7,\r\n",
      "luis,2,\r\n",
      "pedro,5,\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /user/root/output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de la nota media con Scripts de Python ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# Leemos línea a línea de la entrada estándar\n",
    "for line in sys.stdin:  \n",
    "    # Extraemos el nombre y las notas\n",
    "    name, *marks = line.split()\n",
    "    \n",
    "    # Procesamos nota a nota\n",
    "    for mark in marks:\n",
    "        print(f'{name}\\t{mark}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana\t10\r\n",
      "ana\t5\r\n",
      "ana\t6\r\n",
      "ana\t7\r\n",
      "ana\t7\r\n",
      "luis\t0\r\n",
      "luis\t3\r\n",
      "luis\t4\r\n",
      "pedro\t1\r\n",
      "pedro\t3\r\n",
      "pedro\t6\r\n",
      "pedro\t7\r\n",
      "pedro\t8\r\n"
     ]
    }
   ],
   "source": [
    "! cat notas.txt | ./mapper.py | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "import sys\n",
    "\n",
    "prev_name=''\n",
    "acc=0\n",
    "n_marks=0\n",
    "\n",
    "# Leemos línea a línea de la entrada estándar\n",
    "for line in sys.stdin: \n",
    "    \n",
    "    name, mark = line.split()\n",
    "    \n",
    "    # Si el nombre es igual al de la anterior línea o es la primera iteración, acumulamos la suma de notas y el nḿero de notas\n",
    "    if not prev_name or prev_name == name:                \n",
    "        n_marks = n_marks + 1\n",
    "        acc = acc + float(mark)\n",
    "    \n",
    "    # Cuando el nombre sea diferente, emitimos el nombre anterior,la nota media anterior\n",
    "    else:\n",
    "        print(f'{prev_name}\\t{acc/n_marks}')\n",
    "        acc=float(mark)\n",
    "        n_marks=1\n",
    "    prev_name=name\n",
    "           \n",
    "# Emitimos el nombre y la nota media del último nombre\n",
    "print(f'{prev_name}\\t{acc/n_marks}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana\t7.0\r\n",
      "luis\t2.3333333333333335\r\n",
      "pedro\t5.0\r\n"
     ]
    }
   ],
   "source": [
    "! cat notas.txt | ./mapper.py | sort | ./reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/notas.txt\n",
      "Deleted /user/root/output\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -f -r /user/root/notas.txt /user/root/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -copyFromLocal notas.txt /user/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar] /tmp/streamjob1577710992982196227.jar tmpDir=null\n",
      "2023-01-10 13:16:56,739 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "2023-01-10 13:16:56,907 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "2023-01-10 13:16:57,172 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1673341397441_0011\n",
      "2023-01-10 13:16:57,567 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2023-01-10 13:16:57,676 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2023-01-10 13:16:57,869 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1673341397441_0011\n",
      "2023-01-10 13:16:57,870 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-01-10 13:16:58,045 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-10 13:16:58,046 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-10 13:16:58,108 INFO impl.YarnClientImpl: Submitted application application_1673341397441_0011\n",
      "2023-01-10 13:16:58,148 INFO mapreduce.Job: The url to track the job: http://yarnmaster:8088/proxy/application_1673341397441_0011/\n",
      "2023-01-10 13:16:58,150 INFO mapreduce.Job: Running job: job_1673341397441_0011\n",
      "2023-01-10 13:17:04,274 INFO mapreduce.Job: Job job_1673341397441_0011 running in uber mode : false\n",
      "2023-01-10 13:17:04,276 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-01-10 13:17:09,390 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2023-01-10 13:17:10,397 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-01-10 13:17:15,447 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-01-10 13:17:15,471 INFO mapreduce.Job: Job job_1673341397441_0011 completed successfully\n",
      "2023-01-10 13:17:15,580 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=124\n",
      "\t\tFILE: Number of bytes written=832003\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=276\n",
      "\t\tHDFS: Number of bytes written=42\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5986\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2527\n",
      "\t\tTotal time spent by all map tasks (ms)=5986\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2527\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5986\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2527\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6129664\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2587648\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=13\n",
      "\t\tMap output bytes=92\n",
      "\t\tMap output materialized bytes=130\n",
      "\t\tInput split bytes=184\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=130\n",
      "\t\tReduce input records=13\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=26\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=187\n",
      "\t\tCPU time spent (ms)=1490\n",
      "\t\tPhysical memory (bytes) snapshot=788996096\n",
      "\t\tVirtual memory (bytes) snapshot=7625920512\n",
      "\t\tTotal committed heap usage (bytes)=634388480\n",
      "\t\tPeak Map Physical memory (bytes)=281690112\n",
      "\t\tPeak Map Virtual memory (bytes)=2540728320\n",
      "\t\tPeak Reduce Physical memory (bytes)=225640448\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2544709632\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=42\n",
      "2023-01-10 13:17:15,580 INFO streaming.StreamJob: Output directory: /user/root/output\n"
     ]
    }
   ],
   "source": [
    "! mapred streaming \\\n",
    "    -files /media/notebooks/mapper.py,/media/notebooks/reducer.py \\\n",
    "    -input /user/root/notas.txt \\\n",
    "    -output /user/root/output \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana\t7.0\r\n",
      "luis\t2.3333333333333335\r\n",
      "pedro\t5.0\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /user/root/output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de la nota media con Scripts de MrJob ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting marksMR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile marksMR.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from statistics import mean\n",
    "    \n",
    "#Definimos una clase MrJob\n",
    "class MarksMR(MRJob):\n",
    "        \n",
    "    # Mapper: En esta etapa aún no hay clave (_), el valor lo recibimos en la variable line\n",
    "    def mapper(self, _, line):\n",
    "        #Por cada línea, esta se divide en los campos que forman las columnas\n",
    "        name, *marks = line.split()\n",
    "        for mark in marks:            \n",
    "            yield name, float(mark)\n",
    "         \n",
    "    #Reducer: La clave será el nombre y los valores las notas\n",
    "    def reducer(self, name, marks):\n",
    "        yield name, mean(marks)\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    MarksMR.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x marksMR.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/marksMR.root.20230110.121720.351878\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/marksMR.root.20230110.121720.351878/output\n",
      "Streaming final output from /tmp/marksMR.root.20230110.121720.351878/output...\n",
      "\"ana\"\t7.0\n",
      "\"pedro\"\t5.0\n",
      "\"luis\"\t2.3333333333333335\n",
      "Removing temp directory /tmp/marksMR.root.20230110.121720.351878...\n"
     ]
    }
   ],
   "source": [
    "! python3 marksMR.py notas.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/marksMR.root.20230110.121721.382756\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/marksMR.root.20230110.121721.382756/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/marksMR.root.20230110.121721.382756/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar7838062307598998919/] [] /tmp/streamjob16985964687620126.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1673341397441_0012\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1673341397441_0012\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1673341397441_0012\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1673341397441_0012/\n",
      "  Running job: job_1673341397441_0012\n",
      "  Job job_1673341397441_0012 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1673341397441_0012 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/marksMR.root.20230110.121721.382756/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=48\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=176\n",
      "\t\tFILE: Number of bytes written=834435\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=276\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=48\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6922240\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2924544\n",
      "\t\tTotal time spent by all map tasks (ms)=6760\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6760\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2856\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2856\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6760\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2856\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1630\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=191\n",
      "\t\tInput split bytes=184\n",
      "\t\tMap input records=7\n",
      "\t\tMap output bytes=144\n",
      "\t\tMap output materialized bytes=182\n",
      "\t\tMap output records=13\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=282423296\n",
      "\t\tPeak Map Virtual memory (bytes)=2539917312\n",
      "\t\tPeak Reduce Physical memory (bytes)=193245184\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2545160192\n",
      "\t\tPhysical memory (bytes) snapshot=755109888\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=13\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=182\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=26\n",
      "\t\tTotal committed heap usage (bytes)=616038400\n",
      "\t\tVirtual memory (bytes) snapshot=7623725056\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/marksMR.root.20230110.121721.382756/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/marksMR.root.20230110.121721.382756/output...\n",
      "\"ana\"\t7.0\n",
      "\"luis\"\t2.3333333333333335\n",
      "\"pedro\"\t5.0\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/marksMR.root.20230110.121721.382756...\n",
      "Removing temp directory /tmp/marksMR.root.20230110.121721.382756...\n"
     ]
    }
   ],
   "source": [
    "! python3 marksMR.py -r hadoop hdfs:///user/root/notas.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de La Liga de fútbol con MapReduce ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-10 13:18:02--  https://www.football-data.co.uk/mmz4281/2122/SP1.csv\n",
      "Resolving www.football-data.co.uk (www.football-data.co.uk)... ::ffff:217.160.0.246, 217.160.0.246\n",
      "Connecting to www.football-data.co.uk (www.football-data.co.uk)|::ffff:217.160.0.246|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 172174 (168K) [text/csv]\n",
      "Saving to: ‘laliga2122.csv’\n",
      "\n",
      "laliga2122.csv      100%[===================>] 168.14K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-01-10 13:18:02 (1.10 MB/s) - ‘laliga2122.csv’ saved [172174/172174]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O laliga2122.csv https://www.football-data.co.uk/mmz4281/2122/SP1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Div,Date,Time,HomeTeam,AwayTeam,FTHG,FTAG,FTR,HTHG,HTAG,HTR,HS,AS,HST,AST,HF,AF,HC,AC,HY,AY,HR,AR,B365H,B365D,B365A,BWH,BWD,BWA,IWH,IWD,IWA,PSH,PSD,PSA,WHH,WHD,WHA,VCH,VCD,VCA,MaxH,MaxD,MaxA,AvgH,AvgD,AvgA,B365>2.5,B365<2.5,P>2.5,P<2.5,Max>2.5,Max<2.5,Avg>2.5,Avg<2.5,AHh,B365AHH,B365AHA,PAHH,PAHA,MaxAHH,MaxAHA,AvgAHH,AvgAHA,B365CH,B365CD,B365CA,BWCH,BWCD,BWCA,IWCH,IWCD,IWCA,PSCH,PSCD,PSCA,WHCH,WHCD,WHCA,VCCH,VCCD,VCCA,MaxCH,MaxCD,MaxCA,AvgCH,AvgCD,AvgCA,B365C>2.5,B365C<2.5,PC>2.5,PC<2.5,MaxC>2.5,MaxC<2.5,AvgC>2.5,AvgC<2.5,AHCh,B365CAHH,B365CAHA,PCAHH,PCAHA,MaxCAHH,MaxCAHA,AvgCAHH,AvgCAHA\r",
      "\r\n",
      "SP1,13/08/2021,20:00,Valencia,Getafe,1,0,H,1,0,H,4,22,2,4,24,15,1,9,6,3,1,1,2.55,3,3.1,2.65,3,2.95,2.65,2.9,3.05,2.7,3.03,3.11,2.55,3,3,2.63,3,3,2.73,3.2,3.23,2.64,3.01,3.06,2.62,1.5,2.75,1.5,2.75,1.51,2.65,1.49,0,1.82,2.11,1.83,2.11,1.88,2.13,1.81,2.08,2.37,3,3.3,2.45,3,3.25,2.4,2.95,3.4,2.47,3.04,3.48,2.35,3,3.3,2.45,3,3.3,2.57,3.1,3.58,2.42,3,3.34,2.75,1.44,2.84,1.48,2.84,1.51,2.68,1.47,-0.25,2.06,1.87,2.07,1.86,2.1,1.9,2.03,1.84\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head -2 laliga2122.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting laligaMR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile laligaMR.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "    \n",
    "class LaLigaMR(MRJob):\n",
    "        \n",
    "    # Mapper: En esta etapa aún no hay clave (_), el valor lo recibimos en la variable line\n",
    "    def mapper_points(self, _, line):\n",
    "        #Por cada línea, esta se divide en los campos que forman las columnas\n",
    "        _, _, _, home_team, away_team, _, _, result, *rest = line.split(',')\n",
    "        \n",
    "        # Si es la cabecera no emitimos nada\n",
    "        if home_team == \"HomeTeam\":\n",
    "            return\n",
    "        \n",
    "        if result == 'D':            \n",
    "            yield home_team, 1\n",
    "            yield away_team, 1\n",
    "        elif result == 'H':\n",
    "            yield home_team, 3\n",
    "        else:\n",
    "            yield away_team, 3\n",
    "            \n",
    "    def combiner_points(self, team, points):\n",
    "        yield team, sum(points)\n",
    "            \n",
    "    def reducer_points(self, team, points):\n",
    "        yield None, (team, sum(points))\n",
    "        \n",
    "    def reducer_classification(self, _, points):\n",
    "        yield None, sorted(points, key=lambda t: t[1], reverse=True)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_points,\n",
    "                   combiner=self.combiner_points,\n",
    "                   reducer=self.reducer_points),\n",
    "            MRStep(reducer=self.reducer_classification)\n",
    "        ]\n",
    "         \n",
    "if __name__=='__main__':\n",
    "    LaLigaMR.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/laligaMR.root.20230110.121804.417346\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/laligaMR.root.20230110.121804.417346/output\n",
      "Streaming final output from /tmp/laligaMR.root.20230110.121804.417346/output...\n",
      "null\t[[\"Real Madrid\", 86], [\"Barcelona\", 73], [\"Ath Madrid\", 71], [\"Sevilla\", 70], [\"Betis\", 65], [\"Sociedad\", 62], [\"Villarreal\", 59], [\"Ath Bilbao\", 55], [\"Valencia\", 48], [\"Osasuna\", 47], [\"Celta\", 46], [\"Vallecano\", 42], [\"Elche\", 42], [\"Espanol\", 42], [\"Mallorca\", 39], [\"Getafe\", 39], [\"Cadiz\", 39], [\"Granada\", 38], [\"Levante\", 35], [\"Alaves\", 31]]\n",
      "Removing temp directory /tmp/laligaMR.root.20230110.121804.417346...\n"
     ]
    }
   ],
   "source": [
    "! python3 laligaMR.py laliga2122.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/laligaMR.root.20230110.121805.496778\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/laligaMR.root.20230110.121805.496778/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/laligaMR.root.20230110.121805.496778/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6381869742560478812/] [] /tmp/streamjob6528953334619204783.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1673341397441_0013\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1673341397441_0013\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1673341397441_0013\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1673341397441_0013/\n",
      "  Running job: job_1673341397441_0013\n",
      "  Job job_1673341397441_0013 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1673341397441_0013 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaMR.root.20230110.121805.496778/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=176270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=430\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=625\n",
      "\t\tFILE: Number of bytes written=836869\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=176570\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=430\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=11602944\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3065856\n",
      "\t\tTotal time spent by all map tasks (ms)=11331\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=11331\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2994\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2994\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=11331\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2994\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2090\n",
      "\t\tCombine input records=491\n",
      "\t\tCombine output records=40\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=333\n",
      "\t\tInput split bytes=300\n",
      "\t\tMap input records=381\n",
      "\t\tMap output bytes=6230\n",
      "\t\tMap output materialized bytes=631\n",
      "\t\tMap output records=491\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=330203136\n",
      "\t\tPeak Map Virtual memory (bytes)=2546561024\n",
      "\t\tPeak Reduce Physical memory (bytes)=193282048\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2545700864\n",
      "\t\tPhysical memory (bytes) snapshot=817950720\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce input records=40\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=631\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=80\n",
      "\t\tTotal committed heap usage (bytes)=643825664\n",
      "\t\tVirtual memory (bytes) snapshot=7632646144\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar5185449841068187656/] [] /tmp/streamjob2880575829857684195.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1673341397441_0014\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1673341397441_0014\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1673341397441_0014\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1673341397441_0014/\n",
      "  Running job: job_1673341397441_0014\n",
      "  Job job_1673341397441_0014 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1673341397441_0014 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaMR.root.20230110.121805.496778/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=645\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=356\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=476\n",
      "\t\tFILE: Number of bytes written=835029\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=959\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=356\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6274048\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2768896\n",
      "\t\tTotal time spent by all map tasks (ms)=6127\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6127\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2704\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2704\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6127\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2704\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1710\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=192\n",
      "\t\tInput split bytes=314\n",
      "\t\tMap input records=20\n",
      "\t\tMap output bytes=430\n",
      "\t\tMap output materialized bytes=482\n",
      "\t\tMap output records=20\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=329453568\n",
      "\t\tPeak Map Virtual memory (bytes)=2540167168\n",
      "\t\tPeak Reduce Physical memory (bytes)=195649536\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2545242112\n",
      "\t\tPhysical memory (bytes) snapshot=808677376\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=482\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=40\n",
      "\t\tTotal committed heap usage (bytes)=641204224\n",
      "\t\tVirtual memory (bytes) snapshot=7623712768\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/laligaMR.root.20230110.121805.496778/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/laligaMR.root.20230110.121805.496778/output...\n",
      "null\t[[\"Real Madrid\", 86], [\"Barcelona\", 73], [\"Ath Madrid\", 71], [\"Sevilla\", 70], [\"Betis\", 65], [\"Sociedad\", 62], [\"Villarreal\", 59], [\"Ath Bilbao\", 55], [\"Valencia\", 48], [\"Osasuna\", 47], [\"Celta\", 46], [\"Vallecano\", 42], [\"Espanol\", 42], [\"Elche\", 42], [\"Mallorca\", 39], [\"Getafe\", 39], [\"Cadiz\", 39], [\"Granada\", 38], [\"Levante\", 35], [\"Alaves\", 31]]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/laligaMR.root.20230110.121805.496778...\n",
      "Removing temp directory /tmp/laligaMR.root.20230110.121805.496778...\n"
     ]
    }
   ],
   "source": [
    "! python3 laligaMR.py -r hadoop laliga2122.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting laligaMR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile laligaMR.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from datetime import datetime\n",
    "    \n",
    "class LaLigaMR(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "        \n",
    "    # Mapper: En esta etapa aún no hay clave (_), el valor lo recibimos en la variable line\n",
    "    def mapper_points(self, _, line):\n",
    "        #Por cada línea, esta se divide en los campos que forman las columnas\n",
    "        _, date, _, home_team, away_team, _, _, result, *rest = line.split(',')\n",
    "        \n",
    "        # Si es la cabecera no emitimos nada\n",
    "        if home_team == \"HomeTeam\":\n",
    "            return\n",
    "        \n",
    "        date = datetime.strptime(date, \"%d/%m/%Y\").strftime(\"%Y/%m/%d\")\n",
    "\n",
    "        if result == 'D':            \n",
    "            yield home_team, (date, 1)\n",
    "            yield away_team, (date, 1)\n",
    "        elif result == 'H':\n",
    "            yield home_team, (date, 3)\n",
    "            yield away_team, (date, 0)\n",
    "        else:\n",
    "            yield home_team, (date, 0)\n",
    "            yield away_team, (date, 3)\n",
    "            \n",
    "    def reducer_points(self, team, points):\n",
    "        points = list(points)\n",
    "        points = [p for date, p in points]\n",
    "        five_latest_points = points[-5:]\n",
    "        five_latest_points.reverse()\n",
    "        yield None, (team, sum(points), five_latest_points)\n",
    "    \n",
    "    \n",
    "    def reducer_classification(self, _, points):\n",
    "            yield None, sorted(points, key=lambda t: t[1], reverse=True)\n",
    "            \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_points, reducer=self.reducer_points),\n",
    "            MRStep(reducer=self.reducer_classification)\n",
    "        ]\n",
    "         \n",
    "if __name__=='__main__':\n",
    "    LaLigaMR.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/laligaMR.root.20230110.121913.350059\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/laligaMR.root.20230110.121913.350059/output\n",
      "Streaming final output from /tmp/laligaMR.root.20230110.121913.350059/output...\n",
      "null\t[[\"Real Madrid\", 86, [1, 1, 3, 0, 3]], [\"Barcelona\", 73, [0, 1, 3, 3, 3]], [\"Ath Madrid\", 71, [3, 1, 3, 3, 0]], [\"Sevilla\", 70, [3, 1, 1, 1, 1]], [\"Betis\", 65, [1, 3, 3, 0, 1]], [\"Sociedad\", 62, [0, 3, 3, 0, 1]], [\"Villarreal\", 59, [3, 0, 3, 1, 0]], [\"Ath Bilbao\", 55, [0, 3, 0, 1, 3]], [\"Valencia\", 48, [3, 1, 0, 1, 1]], [\"Osasuna\", 47, [0, 0, 1, 1, 1]], [\"Celta\", 46, [0, 3, 0, 3, 1]], [\"Elche\", 42, [3, 0, 0, 0, 1]], [\"Espanol\", 42, [1, 1, 0, 1, 0]], [\"Vallecano\", 42, [0, 0, 0, 1, 1]], [\"Cadiz\", 39, [3, 1, 0, 3, 1]], [\"Getafe\", 39, [0, 1, 1, 1, 1]], [\"Mallorca\", 39, [3, 3, 1, 0, 0]], [\"Granada\", 38, [1, 0, 3, 3, 1]], [\"Levante\", 35, [3, 3, 0, 3, 1]], [\"Alaves\", 31, [0, 0, 3, 0, 3]]]\n",
      "Removing temp directory /tmp/laligaMR.root.20230110.121913.350059...\n"
     ]
    }
   ],
   "source": [
    "! python3 laligaMR.py laliga2122.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
