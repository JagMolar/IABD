{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guía práctica de uso de MapReduce\n",
    "\n",
    "En la sesión práctica se presentarán los siguientes contenidos:\n",
    "\n",
    "* Revisión del `clúster` de Hadoop levantado con `docker-compose`.\n",
    "* Cálculo de la nota media con MapReduce.\n",
    "* Clasificación de La Liga de fútbol con MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisión del `clúster` de Hadoop levantado con `docker-compose`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levantamos la infraestructura del `clúster` de Hadoop que ya se utilizó en la guía práctica anterior. Para\n",
    "ello abrimos un terminal y ejecutamos:\n",
    "\n",
    "```bash\n",
    "docker-compose -f hadoop.yml up\n",
    "```\n",
    "\n",
    "Deberíamos ver algo así:\n",
    "\n",
    "![docker-compose up](./img/docker-compose.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En otro terminal comprobamos que se han levantado los seis contenedores ejecutando:\n",
    "\n",
    "```bash\n",
    "docker-compose -f hadoop.yml ps\n",
    "```\n",
    "\n",
    "Deberíamos ver algo similar a esto:\n",
    "\n",
    "![docker-compose ps](./img/docker-compose-ps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, hay seis contenedores con las siguientes configuraciones:\n",
    "\n",
    "* namenode: es el maestro del sistema de ficheros distribuido HDFS. También tiene el servidor de Jupyter en el que crearemos los `notebooks`.\n",
    "* yarnmaster: es el ResourceManager de YARN.\n",
    "* guia_datanode_1, guia_datanode_2, guia_datanode_3, guia_datanode_4: son 4 contenedores que realizan la función de `datanodes` de HDFS y simultáneamente de `nodeManagers` de YARN. Se puede controlar el número de contenedores creados modificando el parámetro `replicas` del servicio `datanode` en el fichero `hadoop.yml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los siguientes apartados vamos a explicar brevemente como acceder por Web y por línea de comandos a estos contenedores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaz Web de HDFS\n",
    "\n",
    "HDFS tiene una [interfaz Web](http://localhost:9870) que se ha mapeado en el puerto 9870 del equipo anfitrión.\n",
    "Abrimos un navegador y copiamos en la barra de direcciones:\n",
    "\n",
    "```\n",
    "http://localhost:9870\n",
    "```\n",
    "\n",
    "Deberíamos ver algo similar a esto:\n",
    "\n",
    "![hdfs overview](./img/hdfs-overview.png)\n",
    "\n",
    "En la sección `summary` podemos ver en número de `datanodes`:\n",
    "\n",
    "![hdfs summary](./img/hdfs-summary.png)\n",
    "\n",
    "Pulsando sobre en enlace `Live Nodes` o sobre la pestaña `Datanodes` accedemos a la\n",
    "información de cada `datanode`.\n",
    "\n",
    "![hdfs datanodes](./img/hdfs-datanodes.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Línea de comandos HDFS\n",
    "\n",
    "El sistema de ficheros HDFS distribuido admite una serie de comandos que podemos ejecutar desde el terminal. Debemos abrir un terminal en el contenedor de `docker` en el que se ejecuta el `namenode`. Para hacerlo, abrimos otro terminal y ejecutamos:\n",
    "\n",
    "```bash\n",
    "docker exec -it namenode bash\n",
    "```\n",
    "\n",
    "En el contenedor, con el comando:\n",
    "\n",
    "```bash\n",
    "hadoop fs -help\n",
    "```\n",
    "\n",
    "se listan todos los comandos que se pueden ejecutar con HDFS:\n",
    "\n",
    "\n",
    "![hdfs help](./img/hdfs-help.png)\n",
    "\n",
    "Muchos de los comandos de la relación anterior te resultarán familiares ya que se corresponden con los equivalentes de la `bash` de Linux: `-cat`, `-rm`, `-mkdir`, `-ls`, `-head`, ....\n",
    "\n",
    "No te preocupes si no sabes para qué se utilizan estos comandos ya que usaremos algunos de ellos en la parte práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad podemos ejecutar HDFS en cualquiera de los nodos del `clúster` de Hadoop. Puedes\n",
    "probarlo accediendo con `docker exec` a cualquiera de los contenedores y ejecutando desde Jupyter la siguiente instrucción que listará el directorio raíz de sistema distribuido HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxrwxrwx   - root supergroup          0 2022-12-04 16:18 /tmp\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-04 16:18 /user\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el uso de HDFS es importante diferenciar el sistema de ficheros local del sistema de ficheros distribuido. Por ejemplo, el siguiente comando es un simple comando de la `Bash` que lista la raíz del directorio local del contenedor. Observa que la salida es diferente de la instrucción anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app  boot     dev  hdfs  lib\tlib64\tmedia  opt   root  sbin  sys  usr\r\n",
      "bin  dataset  etc  home  lib32\tlibx32\tmnt    proc  run   srv\t tmp  var\r\n"
     ]
    }
   ],
   "source": [
    "! ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaz Web de Yarn\n",
    "\n",
    "El contenedor [`yarnmaster`](http://localhost:8088) también tiene una interfaz Web. Para acceder a ella debemos abrir en el navegador la siguiente dirección:\n",
    "\n",
    "```\n",
    "http://localhost:8088\n",
    "```\n",
    "\n",
    "Veremos lo siguiente:\n",
    "\n",
    "![YARN](./img/yarn.png)\n",
    "\n",
    "La información presentada no es muy interesante ya que todavía no hemos ejecutado ningún trabajo. Posteriormente, cuando realicemos algún proceso MapReduce, podrás comprobar el estado de ejecución de los trabajos en esta dirección."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce\n",
    "\n",
    "Es el modelo de programación que usa Hadoop. Está inspirado en el paradigma de programación funcional. Permite procesar grandes cantidades de datos al realizar una computación paralela y distribuida. La filosofía de `MapReduce` es que los datos se encuentren distribuidos en los nodos en los que se van a computar, evitando costosas operaciones de copiado de datos.  Básicamente se compone de tres fases:\n",
    "\n",
    "* Map: En esta fase se transforman y filtran los datos de forma paralela. En esta fase no podemos hacer ninguna suposición de donde se encuentran los datos que se van a procesar ni del nodo que los va a procesar. La salida de la fase `map` es una tupla `clave, valor`.\n",
    "* Shuffle & sort: Tras la fase `map`, se realiza la reunión de todos los datos que son ordenados según la clave.\n",
    "* Reduce: Resume los resultados de la fase `map`.\n",
    "\n",
    "Ninguna de las fases es obligatoria (pude haber un `map` sin `reduce` y un `reduce` sin un `map`). Además, a un proceso MapReduce puede seguir otro proceso MapReduce. Veremos ejemplo de todo ello en la parte práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La descripción anterior puede resultar un tanto abstracta. Por eso vamos a implementar varios ejemplos prácticos que nos permitan entender mejor como funciona MapReduce. Como ya se ha dicho, la implementación nativa de MapReduce está hecha en Java. Para evitar la complejidad que tendría\n",
    "usar MapReduce con Java, vamos a hacer los ejemplos con [Hadoop Streaming](https://hadoop.apache.org/docs/r1.2.1/streaming.html). Hadoop Streaming permite ejecutar trabajos MapReduce con cualquier proceso capaz de leer de la entrada estándar, con lo que virtualmente podemos usarlo con cualquier lenguaje de programación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo de la nota media con MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que queremos saber la nota media de una serie de alumnos ordenada de mayor a menor nota media. Creamos un fichero llamado `notas.txt` de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"pedro 6 7\\nluis 0 4\\nana 7\\npedro 8 1 3\\nana 5 6 7\\nana 10\\nluis 3\" > notas.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pedro 6 7\r\n",
      "luis 0 4\r\n",
      "ana 7\r\n",
      "pedro 8 1 3\r\n",
      "ana 5 6 7\r\n",
      "ana 10\r\n",
      "luis 3\r\n"
     ]
    }
   ],
   "source": [
    "! cat notas.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, en cada fila aparece el nombre del alumno y una lista de notas separadas por espacios. Un mismo alumno puede aparecer en varias filas y una fila puede tener varias notas. Se trata de calcular la nota media por alumno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Vamos a realizar el mismo proceso de tres formas diferentes:\n",
    "\n",
    "* Usando la `Bash` de Linux.\n",
    "* Usando Python.\n",
    "* Usando una librería de Python para MapReduce denominada `mrjob`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de la nota media con MapReduce con `scripts` de la `Bash`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio vamos a implementar el `map` y el `reduce` en la `bash`. No te preocupes si no\n",
    "entiendes alguno de los comandos ya que realizaremos la misma implementación con Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un fichero el fichero que hará la función `map`. Imprimirá en la consola una línea con el nombre del alumno y cada una de las notas que haya obtenido. La clave será el nombre del alumno y su valor la nota obtenida. Hemos separado nombre el nombre del alumno y la nota mediante una coma. El delimitador por defecto que usa Hadoop no es la coma sino el tabulador. Más adelante verás que tenemos que indicar a MapReduce que use como delimitador la coma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# Leemos línea a línea de la entrada estándar\n",
    "while read line; do\n",
    "    \n",
    "    # Extraemos el nombre de la línea\n",
    "    name=${line%% *}\n",
    "    \n",
    "    # Procesamos nota a nota\n",
    "    for mark in ${line#* }; do\n",
    "                  \n",
    "        # para cada nota emitimos nombre,nota\n",
    "        echo -e \"$name,$mark\"\n",
    "    done    \n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damos permiso de ejecución y lo probamos desde la `Bash`. Siempre es conveniente que pruebes tus programas antes de enviarlos a Hadoop ya que te será más fácil depurar y corregir los errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x mapper.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos el `script` para comprobar los resultados que produce. Hacemos la ejecución en la `Bash` pero siempre teniendo presente que cuando se ejecute en un `clúster` de Hadoop no se puede garantizar qué nodo procesará cada línea del proceso `map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pedro,6\r\n",
      "pedro,7\r\n",
      "luis,0\r\n",
      "luis,4\r\n",
      "ana,7\r\n",
      "pedro,8\r\n",
      "pedro,1\r\n",
      "pedro,3\r\n",
      "ana,5\r\n",
      "ana,6\r\n",
      "ana,7\r\n",
      "ana,10\r\n",
      "luis,3\r\n"
     ]
    }
   ],
   "source": [
    "! cat notas.txt | ./mapper.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de la nota media con MapReduce con `scripts` de `Bash`\n",
    "\n",
    "Ahora creamos el `reduce`. Tampoco podemos saber qué nodos van a procesar los datos, pero en el caso del `reducer`, tenemos garantizado que todas las claves con el mismo valor serán procesadas en el mismo nodo. La dificultad estriba en que un mismo nodo podría recibir los datos de varias claves, aunque siempre estarán ordenados por clave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.sh\n",
    "#!/bin/bash\n",
    "prev_name=\n",
    "acc=0\n",
    "n_marks=0\n",
    "\n",
    "# Leemos línea a línea\n",
    "while read line; do\n",
    "    # Extraemos el nombre y la nota\n",
    "    name=${line%,*}\n",
    "    mark=${line#*,}\n",
    "    \n",
    "    # Si el nombre es igual al de la anterior línea o es la primera iteración, acumulamos la suma de notas y el nḿero de notas\n",
    "    if [ -z \"$prev_name\" -o \"$prev_name\" == \"$name\" ]; then                \n",
    "        let n_marks++\n",
    "        acc=$(($acc + $mark))\n",
    "    \n",
    "    # Cuando el nombre sea diferente, emitimos el nombre anterior,la nota media anterior\n",
    "    else\n",
    "        echo $prev_name,$(($acc/n_marks))\n",
    "        acc=$mark\n",
    "        n_marks=1\n",
    "    fi\n",
    "    prev_name=$name\n",
    "done\n",
    "           \n",
    "# Emitimos el nombre y la nota media del último nombre\n",
    "echo $prev_name,$(($acc/n_marks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damos permisos de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x reducer.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos el fichero en la `Bash` antes de ejecutarlo en Hadoop. Entre el proceso `map` y el `reduce`, Hadoop ordenará el fichero por clave y repartirá las claves entre nodos del `clúster`. Nosotros simulamos este proceso con una tubería `sort` intermedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana,7\r\n",
      "luis,2\r\n",
      "pedro,5\r\n"
     ]
    }
   ],
   "source": [
    "! cat notas.txt | ./mapper.sh | sort | ./reducer.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que se ha calculado la nota media sin decimales. Esta es una debilidad de nuestra implementación en la `Bash` que se solucionará posteriormente en la implementación con Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya casi estamos preparados para ejecutar el proceso en Hadoop. Para poder trabajar con Hadoop, el fichero de datos hay que \"subirlo\" a HDFS. También hay que indicar el directorio de salida del proceso MapReduce. En posteriores ejecuciones, si vuelves a subir el mismo fichero o a utilizar el mismo directorio de salida, se producirá un error. Por eso lo primero que vamos a hacer es asegurarnos de borrar el fichero de datos y el directorio de salida en caso de que ya existieran. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/notas.txt\r\n",
      "Deleted /user/root/output\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -f -r /user/root/notas.txt /user/root/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiamos el fichero de notas del equipo local a HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -copyFromLocal notas.txt /user/root/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listamos el directorio para ver el archivo copiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 root supergroup         61 2022-12-04 21:09 /user/root/notas.txt\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida es similar a la que produce el comando `ls -l` en la `Bash`. El número 3 que ves sin embargo tiene un significado diferente. En este caso lo que indica el 3 es que el fichero se ha replicado en tres nodos. 3 es el factor de replicación por defecto en Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos examinar el fichero `notas.txt` en HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pedro 6 7\r\n",
      "luis 0 4\r\n",
      "ana 7\r\n",
      "pedro 8 1 3\r\n",
      "ana 5 6 7\r\n",
      "ana 10\r\n",
      "luis 3\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /user/root/notas.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos conocer en qué nodos está almacenado el fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://namenode:9870/fsck?ugi=root&files=1&locations=1&blocks=1&path=%2Fuser%2Froot%2Fnotas.txt\n",
      "FSCK started by root (auth:SIMPLE) from /172.18.0.7 for path /user/root/notas.txt at Sun Dec 04 21:09:07 CET 2022\n",
      "\n",
      "/user/root/notas.txt 61 bytes, replicated: replication=3, 1 block(s):  OK\n",
      "0. BP-858413720-172.17.0.2-1624180586568:blk_1073741849_1025 len=61 Live_repl=3  [DatanodeInfoWithStorage[172.18.0.2:9866,DS-7746e6b9-217a-497e-95c9-d05614f224b3,DISK], DatanodeInfoWithStorage[172.18.0.6:9866,DS-3ba3a233-9506-4e45-864c-c7f3852ee857,DISK], DatanodeInfoWithStorage[172.18.0.3:9866,DS-3b152d79-c3ff-4577-8e69-1d13e3976bc7,DISK]]\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t4\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t0\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t61 B\n",
      " Total files:\t1\n",
      " Total blocks (validated):\t1 (avg. block size 61 B)\n",
      " Minimally replicated blocks:\t1 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t3\n",
      " Average block replication:\t3.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      " Blocks queued for replication:\t0\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      " Blocks queued for replication:\t0\n",
      "FSCK ended at Sun Dec 04 21:09:07 CET 2022 in 1 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/user/root/notas.txt' is HEALTHY\n"
     ]
    }
   ],
   "source": [
    "! hdfs fsck /user/root/notas.txt -files -locations -blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya podemos ejecutar el proceso MapReduce con el programa `mapred`. Observa que hemos indicado que vamos a usar la coma como delimitador; le tenemos que dar la ruta de acceso a los ficheros  `map` y `reduce` y qué función tendrán cada uno; por último, tenemos que indicar el directorio de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar] /tmp/streamjob7345304166545004129.jar tmpDir=null\n",
      "2022-12-04 21:23:14,526 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.18.0.4:8032\n",
      "2022-12-04 21:23:14,810 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.18.0.4:8032\n",
      "2022-12-04 21:23:15,133 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1670167100255_0011\n",
      "2022-12-04 21:23:15,419 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2022-12-04 21:23:15,491 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2022-12-04 21:23:15,517 INFO Configuration.deprecation: mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2022-12-04 21:23:15,625 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1670167100255_0011\n",
      "2022-12-04 21:23:15,625 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2022-12-04 21:23:15,818 INFO conf.Configuration: resource-types.xml not found\n",
      "2022-12-04 21:23:15,818 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2022-12-04 21:23:15,888 INFO impl.YarnClientImpl: Submitted application application_1670167100255_0011\n",
      "2022-12-04 21:23:15,933 INFO mapreduce.Job: The url to track the job: http://yarnmaster:8088/proxy/application_1670167100255_0011/\n",
      "2022-12-04 21:23:15,935 INFO mapreduce.Job: Running job: job_1670167100255_0011\n",
      "2022-12-04 21:23:21,017 INFO mapreduce.Job: Job job_1670167100255_0011 running in uber mode : false\n",
      "2022-12-04 21:23:21,018 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2022-12-04 21:23:26,075 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2022-12-04 21:23:30,097 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2022-12-04 21:23:30,105 INFO mapreduce.Job: Job job_1670167100255_0011 completed successfully\n",
      "2022-12-04 21:23:30,179 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=137\n",
      "\t\tFILE: Number of bytes written=832545\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=276\n",
      "\t\tHDFS: Number of bytes written=24\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5175\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2087\n",
      "\t\tTotal time spent by all map tasks (ms)=5175\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2087\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5175\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2087\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5299200\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2137088\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=13\n",
      "\t\tMap output bytes=105\n",
      "\t\tMap output materialized bytes=143\n",
      "\t\tInput split bytes=184\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce shuffle bytes=143\n",
      "\t\tReduce input records=13\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=26\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=152\n",
      "\t\tCPU time spent (ms)=1710\n",
      "\t\tPhysical memory (bytes) snapshot=794025984\n",
      "\t\tVirtual memory (bytes) snapshot=7680823296\n",
      "\t\tTotal committed heap usage (bytes)=830472192\n",
      "\t\tPeak Map Physical memory (bytes)=302088192\n",
      "\t\tPeak Map Virtual memory (bytes)=2558152704\n",
      "\t\tPeak Reduce Physical memory (bytes)=195100672\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2565931008\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=24\n",
      "2022-12-04 21:23:30,179 INFO streaming.StreamJob: Output directory: /user/root/output\n"
     ]
    }
   ],
   "source": [
    "! mapred streaming \\\n",
    "    -D mapred.textoutputformat.separator=\",\" \\\n",
    "    -files /media/notebooks/mapper.sh,/media/notebooks/reducer.sh \\\n",
    "    -input /user/root/notas.txt \\\n",
    "    -output /user/root/output \\\n",
    "    -mapper mapper.sh \\\n",
    "    -reducer reducer.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras la ejecución podemos mostrar el contenido del directorio de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2022-12-04 21:13 /user/root/output/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup         18 2022-12-04 21:13 /user/root/output/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/root/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si imprimimos el contendido, comprobamos que se ha calculado la media de notas de cada usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana,7,\r\n",
      "luis,2,\r\n",
      "pedro,5,\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /user/root/output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de la nota media con MapReduce con `scripts` de `Python`\n",
    "\n",
    "Podemos traducir los `scripts` de la `Bash` a sintaxis `Python`, que es más fácil e intuitiva. En este caso no vamos a cambiar el delimitador por defecto, que es el tabulador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# Leemos línea a línea de la entrada estándar\n",
    "for line in sys.stdin:  \n",
    "    # Extraemos el nombre y las notas\n",
    "    name, *marks = line.split()\n",
    "    \n",
    "    # Procesamos nota a nota\n",
    "    for mark in marks:\n",
    "        print(f'{name}\\t{mark}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que el `mapper` funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana\t10\r\n",
      "ana\t5\r\n",
      "ana\t6\r\n",
      "ana\t7\r\n",
      "ana\t7\r\n",
      "luis\t0\r\n",
      "luis\t3\r\n",
      "luis\t4\r\n",
      "pedro\t1\r\n",
      "pedro\t3\r\n",
      "pedro\t6\r\n",
      "pedro\t7\r\n",
      "pedro\t8\r\n"
     ]
    }
   ],
   "source": [
    "! cat notas.txt | ./mapper.py | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traducimos también el `reducer` a `Python`. Hay que tener la precaución de convertir las notas a un tipo numérico (`float` en este caso) ya que `split()` las devuelve como cadenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "import sys\n",
    "\n",
    "prev_name=''\n",
    "acc=0\n",
    "n_marks=0\n",
    "\n",
    "# Leemos línea a línea de la entrada estándar\n",
    "for line in sys.stdin: \n",
    "    \n",
    "    name, mark = line.split()\n",
    "    \n",
    "    # Si el nombre es igual al de la anterior línea o es la primera iteración, acumulamos la suma de notas y el nḿero de notas\n",
    "    if not prev_name or prev_name == name:                \n",
    "        n_marks = n_marks + 1\n",
    "        acc = acc + float(mark)\n",
    "    \n",
    "    # Cuando el nombre sea diferente, emitimos el nombre anterior,la nota media anterior\n",
    "    else:\n",
    "        print(f'{prev_name}\\t{acc/n_marks}')\n",
    "        acc=float(mark)\n",
    "        n_marks=1\n",
    "    prev_name=name\n",
    "           \n",
    "# Emitimos el nombre y la nota media del último nombre\n",
    "print(f'{prev_name}\\t{acc/n_marks}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damos permisos de ejecución y probamos el `script`. Vemos que se calcula correctamente la media corrigiendo el problema de los decimales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana\t7.0\r\n",
      "luis\t2.3333333333333335\r\n",
      "pedro\t5.0\r\n"
     ]
    }
   ],
   "source": [
    "! cat notas.txt | ./mapper.py | sort | ./reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya podemos ejecutar el proceso MapReduce en Hadoop. Haciendo exactamente lo mismo que en el caso de la implementación con la `Bash`:\n",
    "\n",
    "* Borramos el fichero `notas.txt` y el directorio de salida.\n",
    "* Copiamos el ficheros `notas.txt` en HDFS.\n",
    "* Llamamos al programa `mapred`.\n",
    "* Examinamos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/notas.txt\r\n",
      "Deleted /user/root/output\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -f -r /user/root/notas.txt /user/root/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -copyFromLocal notas.txt /user/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar] /tmp/streamjob8235456726287670683.jar tmpDir=null\n",
      "2023-01-10 11:04:28,127 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "2023-01-10 11:04:28,447 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "2023-01-10 11:04:28,680 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1673341397441_0002\n",
      "2023-01-10 11:04:29,110 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2023-01-10 11:04:29,210 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2023-01-10 11:04:29,366 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1673341397441_0002\n",
      "2023-01-10 11:04:29,366 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-01-10 11:04:29,594 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-10 11:04:29,595 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-10 11:04:29,688 INFO impl.YarnClientImpl: Submitted application application_1673341397441_0002\n",
      "2023-01-10 11:04:29,737 INFO mapreduce.Job: The url to track the job: http://yarnmaster:8088/proxy/application_1673341397441_0002/\n",
      "2023-01-10 11:04:29,739 INFO mapreduce.Job: Running job: job_1673341397441_0002\n",
      "2023-01-10 11:04:36,947 INFO mapreduce.Job: Job job_1673341397441_0002 running in uber mode : false\n",
      "2023-01-10 11:04:36,950 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-01-10 11:05:09,017 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-01-10 11:05:15,077 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-01-10 11:05:16,104 INFO mapreduce.Job: Job job_1673341397441_0002 completed successfully\n",
      "2023-01-10 11:05:16,217 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=124\n",
      "\t\tFILE: Number of bytes written=832003\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=276\n",
      "\t\tHDFS: Number of bytes written=42\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=49242\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3409\n",
      "\t\tTotal time spent by all map tasks (ms)=49242\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3409\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=49242\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3409\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=50423808\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3490816\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=13\n",
      "\t\tMap output bytes=92\n",
      "\t\tMap output materialized bytes=130\n",
      "\t\tInput split bytes=184\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=130\n",
      "\t\tReduce input records=13\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=26\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=484\n",
      "\t\tCPU time spent (ms)=13320\n",
      "\t\tPhysical memory (bytes) snapshot=809127936\n",
      "\t\tVirtual memory (bytes) snapshot=7625560064\n",
      "\t\tTotal committed heap usage (bytes)=632291328\n",
      "\t\tPeak Map Physical memory (bytes)=326709248\n",
      "\t\tPeak Map Virtual memory (bytes)=2540646400\n",
      "\t\tPeak Reduce Physical memory (bytes)=195190784\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2545311744\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=42\n",
      "2023-01-10 11:05:16,217 INFO streaming.StreamJob: Output directory: /user/root/output\n"
     ]
    }
   ],
   "source": [
    "! mapred streaming \\\n",
    "    -files /media/notebooks/mapper.py,/media/notebooks/reducer.py \\\n",
    "    -input /user/root/notas.txt \\\n",
    "    -output /user/root/output \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana\t7.0\r\n",
      "luis\t2.3333333333333335\r\n",
      "pedro\t5.0\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -cat /user/root/output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo de la nota media con MapReduce con `mrjob`\n",
    "\n",
    "[`mrjob`](https://mrjob.readthedocs.io/en/latest/) es una librería de Python que facilita enormemente el proceso MapReduce.\n",
    "\n",
    "En `mrjob` simplemente tenemos que escribir un único fichero que contenga una clase que extienda\n",
    "`MRJob`y que defina los métodos `mapper` y `reduce`. Ninguno de los dos métodos es obligatorio. \n",
    "\n",
    "El método `mapper` inicial será llamado una vez por cada línea con dos parámetros: una clave `None` y el valor con el contenido completo de la línea. El `mapper` debe procesar la línea extrayendo la información que requiera y emitiendo (usando `yield`) una tupla \"clave, valor\". Aunque no es imprescindible puedes ampliar información sobre los generadores de Python y lo que significa `yield` [aquí](https://realpython.com/introduction-to-python-generators/).\n",
    "\n",
    "El método `reduce` recibe la clave emitida por el `mapper` y un iterador con los valores de esa clave. Al recibir todos los valores de cada clave en un único parámetro se simplifica enormemente el proceso que realizamos en los apartados anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing marksMR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile marksMR.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from statistics import mean\n",
    "    \n",
    "#Definimos una clase MrJob\n",
    "class MarksMR(MRJob):\n",
    "        \n",
    "    # Mapper: En esta etapa aún no hay clave (_), el valor lo recibimos en la variable line\n",
    "    def mapper(self, _, line):\n",
    "        #Por cada línea, esta se divide en los campos que forman las columnas\n",
    "        name, *marks = line.split()\n",
    "        for mark in marks:            \n",
    "            yield name, float(mark)\n",
    "         \n",
    "    #Reducer: La clave será el nombre y los valores las notas\n",
    "    def reducer(self, name, marks):\n",
    "        yield name, mean(marks)\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    MarksMR.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damos permisos de ejecución al `script`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod ugo+x marksMR.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y lo probamos localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/marksMR.root.20230110.101042.336837\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/marksMR.root.20230110.101042.336837/output\n",
      "Streaming final output from /tmp/marksMR.root.20230110.101042.336837/output...\n",
      "\"ana\"\t7.0\n",
      "\"pedro\"\t5.0\n",
      "\"luis\"\t2.3333333333333335\n",
      "Removing temp directory /tmp/marksMR.root.20230110.101042.336837...\n"
     ]
    }
   ],
   "source": [
    "! python3 marksMR.py notas.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probarlo en el `clúster` de Hadoop simplemente tenemos que añadir una opción. Observa que no es necesario copiar previamente el fichero en HDFS ya que `mrjob` lo hace por nosotros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/marksMR.root.20230110.101118.787064\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/marksMR.root.20230110.101118.787064/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/marksMR.root.20230110.101118.787064/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar7726622170482410347/] [] /tmp/streamjob7406220662964122941.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1673341397441_0003\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1673341397441_0003\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1673341397441_0003\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1673341397441_0003/\n",
      "  Running job: job_1673341397441_0003\n",
      "  Job job_1673341397441_0003 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1673341397441_0003 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/marksMR.root.20230110.101118.787064/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=92\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=48\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=176\n",
      "\t\tFILE: Number of bytes written=834441\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=276\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=48\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=14490624\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4634624\n",
      "\t\tTotal time spent by all map tasks (ms)=14151\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14151\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4526\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4526\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14151\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4526\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2760\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=305\n",
      "\t\tInput split bytes=184\n",
      "\t\tMap input records=7\n",
      "\t\tMap output bytes=144\n",
      "\t\tMap output materialized bytes=182\n",
      "\t\tMap output records=13\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=327946240\n",
      "\t\tPeak Map Virtual memory (bytes)=2541694976\n",
      "\t\tPeak Reduce Physical memory (bytes)=180883456\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2545516544\n",
      "\t\tPhysical memory (bytes) snapshot=792502272\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=13\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=182\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=26\n",
      "\t\tTotal committed heap usage (bytes)=638058496\n",
      "\t\tVirtual memory (bytes) snapshot=7628144640\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/marksMR.root.20230110.101118.787064/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/marksMR.root.20230110.101118.787064/output...\n",
      "\"ana\"\t7.0\n",
      "\"luis\"\t2.3333333333333335\n",
      "\"pedro\"\t5.0\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/marksMR.root.20230110.101118.787064...\n",
      "Removing temp directory /tmp/marksMR.root.20230110.101118.787064...\n"
     ]
    }
   ],
   "source": [
    "! python3 marksMR.py -r hadoop hdfs:///user/root/notas.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de La Liga de fútbol con MapReduce\n",
    "\n",
    "Vamos a hacer un ejercicio un poco más complicado con MapReduce. En esta ocasión se trata de calcular la clasificación de la Liga de Fútbol de Primera División Española de la temporada 2021/2022 a partir de los resultados de los partidos que se disputaron.\n",
    "\n",
    "En [esta Web](https://www.football-data.co.uk/spainm.php) podemos descargar los resultados de los partidos de las últimas temporadas.\n",
    "\n",
    "![www.football-data.co.uk](./img/www.football-data.co.uk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos el fichero de resultados de la temporada 2021/2022 y lo renombramos a `laliga2122.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-10 11:13:24--  https://www.football-data.co.uk/mmz4281/2122/SP1.csv\n",
      "Resolving www.football-data.co.uk (www.football-data.co.uk)... 217.160.0.246, ::ffff:217.160.0.246\n",
      "Connecting to www.football-data.co.uk (www.football-data.co.uk)|217.160.0.246|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 172174 (168K) [text/csv]\n",
      "Saving to: ‘laliga2122.csv’\n",
      "\n",
      "laliga2122.csv      100%[===================>] 168.14K  1.04MB/s    in 0.2s    \n",
      "\n",
      "2023-01-10 11:13:25 (1.04 MB/s) - ‘laliga2122.csv’ saved [172174/172174]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O laliga2122.csv https://www.football-data.co.uk/mmz4281/2122/SP1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos las dos primeras líneas del fichero. Observa que la primera línea es la cabecera y la siguiente es la información sobre un partido de fútbol. Ambas líneas tienen los campos separados por comas (es lo que significa `csv`: \"comma-separated values\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Div,Date,Time,HomeTeam,AwayTeam,FTHG,FTAG,FTR,HTHG,HTAG,HTR,HS,AS,HST,AST,HF,AF,HC,AC,HY,AY,HR,AR,B365H,B365D,B365A,BWH,BWD,BWA,IWH,IWD,IWA,PSH,PSD,PSA,WHH,WHD,WHA,VCH,VCD,VCA,MaxH,MaxD,MaxA,AvgH,AvgD,AvgA,B365>2.5,B365<2.5,P>2.5,P<2.5,Max>2.5,Max<2.5,Avg>2.5,Avg<2.5,AHh,B365AHH,B365AHA,PAHH,PAHA,MaxAHH,MaxAHA,AvgAHH,AvgAHA,B365CH,B365CD,B365CA,BWCH,BWCD,BWCA,IWCH,IWCD,IWCA,PSCH,PSCD,PSCA,WHCH,WHCD,WHCA,VCCH,VCCD,VCCA,MaxCH,MaxCD,MaxCA,AvgCH,AvgCD,AvgCA,B365C>2.5,B365C<2.5,PC>2.5,PC<2.5,MaxC>2.5,MaxC<2.5,AvgC>2.5,AvgC<2.5,AHCh,B365CAHH,B365CAHA,PCAHH,PCAHA,MaxCAHH,MaxCAHA,AvgCAHH,AvgCAHA\r",
      "\r\n",
      "SP1,13/08/2021,20:00,Valencia,Getafe,1,0,H,1,0,H,4,22,2,4,24,15,1,9,6,3,1,1,2.55,3,3.1,2.65,3,2.95,2.65,2.9,3.05,2.7,3.03,3.11,2.55,3,3,2.63,3,3,2.73,3.2,3.23,2.64,3.01,3.06,2.62,1.5,2.75,1.5,2.75,1.51,2.65,1.49,0,1.82,2.11,1.83,2.11,1.88,2.13,1.81,2.08,2.37,3,3.3,2.45,3,3.25,2.4,2.95,3.4,2.47,3.04,3.48,2.35,3,3.3,2.45,3,3.3,2.57,3.1,3.58,2.42,3,3.34,2.75,1.44,2.84,1.48,2.84,1.51,2.68,1.47,-0.25,2.06,1.87,2.07,1.86,2.1,1.9,2.03,1.84\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head -2 laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender el significado de cada campo, la Web tiene un fichero de [`metadata`](https://www.football-data.co.uk/notes.txt). En la imagen se muestran los campos relevantes para el proceso que queremos realizar.\n",
    "\n",
    "![metadata](./img/metadata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretamente, los campos 4º y 5º contienen los nombres de los equipos local y visitante respectivamente y el campo 8º informa cuál de ellos obtuvo la victoria. Así 'H' significa que ganó el equipo local, 'A' que lo hizo el visitante y 'D' que empataron. Sabiendo que el equipo que gana obtiene 3 puntos, el que pierde 0 puntos y si empatan ambos equipos se llevan 1 punto, podemos calcular la clasificación final de la liga."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejercicio lo vamos a resolver únicamente con `mrjob`. En este caso hemos tenido que hacer uso del método `steps` de `mrjob` que permite definir etapas. En este ejercicio es necesario ya que tenemos dos reductores, uno para calcular la suma de los puntos de un equipo y otro para calcular la clasificación. Esta es una descripción del proceso MapReduce:\n",
    "\n",
    "* La primera etapa comienza con el `mapper` al que hemos llamado `mapper_points` que lo que hace es procesar cada línea que corresponde a un partido y extraer los equipos que se enfrentan. Emite como clave el nombre del equipo y como valor los puntos que ha obtenido.\n",
    "* El `combiner_points` es un combinador. Es un proceso que hace una función parecida al reductor y que permite optimizar el funcionamiento ya que hace agregaciones parciales e intermedias antes de enviarlas al reductor.\n",
    "* El `reducer_points` recibe como clave cada equipo y como valor un iterador con los puntos que ha obtenido ese equipo. Emite como clave `None` y como valor una tupla que contiene el nombre del equipo y la suma de los puntos. Al emitir una clave `None` todos las tuplas emitidas serán procesadas en un único reductor en la próxima etapa. Es muy importante asegurar que el volumen de datos que reciba ese reductor sea pequeño.\n",
    "* La segunda etapa sólo consta de un reductor llamado `reducer_classification`. Este reductor ignora la clave ya que no contiene información útil y como valor recibe un iterador de tuplas `equipo,puntos` emitido por el reductor de la primera etapa, `reducer_points`. Lo que hace es emitir una clave nula con los equipos ordenados por puntos de mayor a menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing laligaMR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile laligaMR.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "    \n",
    "class LaLigaMR(MRJob):\n",
    "        \n",
    "    # Mapper: En esta etapa aún no hay clave (_), el valor lo recibimos en la variable line\n",
    "    def mapper_points(self, _, line):\n",
    "        #Por cada línea, esta se divide en los campos que forman las columnas\n",
    "        _, _, _, home_team, away_team, _, _, result, *rest = line.split(',')\n",
    "        \n",
    "        # Si es la cabecera no emitimos nada\n",
    "        if home_team == \"HomeTeam\":\n",
    "            return\n",
    "        \n",
    "        if result == 'D':            \n",
    "            yield home_team, 1\n",
    "            yield away_team, 1\n",
    "        elif result == 'H':\n",
    "            yield home_team, 3\n",
    "        else:\n",
    "            yield away_team, 3\n",
    "            \n",
    "    def combiner_points(self, team, points):\n",
    "        yield team, sum(points)\n",
    "            \n",
    "    def reducer_points(self, team, points):\n",
    "        yield None, (team, sum(points))\n",
    "        \n",
    "    def reducer_classification(self, _, points):\n",
    "        yield None, sorted(points, key=lambda t: t[1], reverse=True)\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_points,\n",
    "                   combiner=self.combiner_points,\n",
    "                   reducer=self.reducer_points),\n",
    "            MRStep(reducer=self.reducer_classification)\n",
    "        ]\n",
    "         \n",
    "if __name__=='__main__':\n",
    "    LaLigaMR.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos el `script` y obtenemos la clasificación final. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/laligaMR.root.20230110.102631.150652\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/laligaMR.root.20230110.102631.150652/output\n",
      "Streaming final output from /tmp/laligaMR.root.20230110.102631.150652/output...\n",
      "null\t[[\"Real Madrid\", 86], [\"Barcelona\", 73], [\"Ath Madrid\", 71], [\"Sevilla\", 70], [\"Betis\", 65], [\"Sociedad\", 62], [\"Villarreal\", 59], [\"Ath Bilbao\", 55], [\"Valencia\", 48], [\"Osasuna\", 47], [\"Celta\", 46], [\"Vallecano\", 42], [\"Elche\", 42], [\"Espanol\", 42], [\"Mallorca\", 39], [\"Getafe\", 39], [\"Cadiz\", 39], [\"Granada\", 38], [\"Levante\", 35], [\"Alaves\", 31]]\n",
      "Removing temp directory /tmp/laligaMR.root.20230110.102631.150652...\n"
     ]
    }
   ],
   "source": [
    "! python3 laligaMR.py laliga2122.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comprobar que la clasificación calculada con MapReduce se corresponde con la [oficial de la temporada 2021/222](https://www.google.com/search?q=clasificacion+liga+2021+2022&oq=clasificacion+liga+2021+2022#sie=lg) con la salvedad de los equipos que han obtenido los mismos puntos en los que la posición oficial puede diferir de la que hemos calculado nosotros. Esto es perfectamente lógico ya que en caso de que dos equipos empaten a puntos, se tienen en cuenta criterios auxiliares como los resultados que han obtenido en enfrentamientos directos o la diferencia de goles, que no ha computado nuestro proceso MapReduce.\n",
    "\n",
    "![clasificación](./img/clasificacion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobada que la ejecución local es correcta, podemos probar la ejecución en el `clúster`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/laligaMR.root.20230110.102718.695888\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/laligaMR.root.20230110.102718.695888/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/laligaMR.root.20230110.102718.695888/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar3788282063341730512/] [] /tmp/streamjob5947904667860205735.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1673341397441_0004\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1673341397441_0004\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1673341397441_0004\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1673341397441_0004/\n",
      "  Running job: job_1673341397441_0004\n",
      "  Job job_1673341397441_0004 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1673341397441_0004 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaMR.root.20230110.102718.695888/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=176270\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=430\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=625\n",
      "\t\tFILE: Number of bytes written=836869\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=176570\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=430\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=18824192\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4491264\n",
      "\t\tTotal time spent by all map tasks (ms)=18383\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=18383\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4386\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4386\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=18383\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4386\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=6370\n",
      "\t\tCombine input records=491\n",
      "\t\tCombine output records=40\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=416\n",
      "\t\tInput split bytes=300\n",
      "\t\tMap input records=381\n",
      "\t\tMap output bytes=6230\n",
      "\t\tMap output materialized bytes=631\n",
      "\t\tMap output records=491\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=324886528\n",
      "\t\tPeak Map Virtual memory (bytes)=2540425216\n",
      "\t\tPeak Reduce Physical memory (bytes)=226537472\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2544693248\n",
      "\t\tPhysical memory (bytes) snapshot=840073216\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce input records=40\n",
      "\t\tReduce output records=20\n",
      "\t\tReduce shuffle bytes=631\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=80\n",
      "\t\tTotal committed heap usage (bytes)=650117120\n",
      "\t\tVirtual memory (bytes) snapshot=7625498624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar432445133909811999/] [] /tmp/streamjob6214510828062890990.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.20.0.6:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1673341397441_0005\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1673341397441_0005\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1673341397441_0005\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1673341397441_0005/\n",
      "  Running job: job_1673341397441_0005\n",
      "  Job job_1673341397441_0005 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1673341397441_0005 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/laligaMR.root.20230110.102718.695888/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=645\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=356\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=476\n",
      "\t\tFILE: Number of bytes written=835029\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=959\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=356\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=7342080\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2767872\n",
      "\t\tTotal time spent by all map tasks (ms)=7170\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7170\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2703\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2703\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7170\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2703\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1860\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=248\n",
      "\t\tInput split bytes=314\n",
      "\t\tMap input records=20\n",
      "\t\tMap output bytes=430\n",
      "\t\tMap output materialized bytes=482\n",
      "\t\tMap output records=20\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=283910144\n",
      "\t\tPeak Map Virtual memory (bytes)=2542514176\n",
      "\t\tPeak Reduce Physical memory (bytes)=224456704\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2544226304\n",
      "\t\tPhysical memory (bytes) snapshot=786423808\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=482\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=40\n",
      "\t\tTotal committed heap usage (bytes)=634388480\n",
      "\t\tVirtual memory (bytes) snapshot=7626932224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/laligaMR.root.20230110.102718.695888/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/laligaMR.root.20230110.102718.695888/output...\n",
      "null\t[[\"Real Madrid\", 86], [\"Barcelona\", 73], [\"Ath Madrid\", 71], [\"Sevilla\", 70], [\"Betis\", 65], [\"Sociedad\", 62], [\"Villarreal\", 59], [\"Ath Bilbao\", 55], [\"Valencia\", 48], [\"Osasuna\", 47], [\"Celta\", 46], [\"Vallecano\", 42], [\"Espanol\", 42], [\"Elche\", 42], [\"Mallorca\", 39], [\"Getafe\", 39], [\"Cadiz\", 39], [\"Granada\", 38], [\"Levante\", 35], [\"Alaves\", 31]]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/laligaMR.root.20230110.102718.695888...\n",
      "Removing temp directory /tmp/laligaMR.root.20230110.102718.695888...\n"
     ]
    }
   ],
   "source": [
    "! python3 laligaMR.py -r hadoop laliga2122.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting laligaMR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile laligaMR.py\n",
    "#!/usr/bin/python3\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from datetime import datetime\n",
    "    \n",
    "class LaLigaMR(MRJob):\n",
    "    \n",
    "    SORT_VALUES = True\n",
    "        \n",
    "    # Mapper: En esta etapa aún no hay clave (_), el valor lo recibimos en la variable line\n",
    "    def mapper_points(self, _, line):\n",
    "        #Por cada línea, esta se divide en los campos que forman las columnas\n",
    "        _, date, _, home_team, away_team, _, _, result, *rest = line.split(',')\n",
    "        \n",
    "        # Si es la cabecera no emitimos nada\n",
    "        if home_team == \"HomeTeam\":\n",
    "            return\n",
    "        \n",
    "        date = datetime.strptime(date, \"%d/%m/%Y\").strftime(\"%Y/%m/%d\")\n",
    "\n",
    "        if result == 'D':            \n",
    "            yield home_team, (date, 1)\n",
    "            yield away_team, (date, 1)\n",
    "        elif result == 'H':\n",
    "            yield home_team, (date, 3)\n",
    "            yield away_team, (date, 0)\n",
    "        else:\n",
    "            yield home_team, (date, 0)\n",
    "            yield away_team, (date, 3)\n",
    "            \n",
    "    def reducer_points(self, team, points):\n",
    "        points = list(points)\n",
    "        points = [p for date, p in points]\n",
    "        five_latest_points = points[-5:]\n",
    "        five_latest_points.reverse()\n",
    "        yield None, (team, sum(points), five_latest_points)\n",
    "    \n",
    "    \n",
    "    def reducer_classification(self, _, points):\n",
    "            yield None, sorted(points, key=lambda t: t[1], reverse=True)\n",
    "            \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_points, reducer=self.reducer_points),\n",
    "            MRStep(reducer=self.reducer_classification)\n",
    "        ]\n",
    "         \n",
    "if __name__=='__main__':\n",
    "    LaLigaMR.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/laligaMR.root.20230110.102837.425932\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/laligaMR.root.20230110.102837.425932/output\n",
      "Streaming final output from /tmp/laligaMR.root.20230110.102837.425932/output...\n",
      "null\t[[\"Real Madrid\", 86, [1, 1, 3, 0, 3]], [\"Barcelona\", 73, [0, 1, 3, 3, 3]], [\"Ath Madrid\", 71, [3, 1, 3, 3, 0]], [\"Sevilla\", 70, [3, 1, 1, 1, 1]], [\"Betis\", 65, [1, 3, 3, 0, 1]], [\"Sociedad\", 62, [0, 3, 3, 0, 1]], [\"Villarreal\", 59, [3, 0, 3, 1, 0]], [\"Ath Bilbao\", 55, [0, 3, 0, 1, 3]], [\"Valencia\", 48, [3, 1, 0, 1, 1]], [\"Osasuna\", 47, [0, 0, 1, 1, 1]], [\"Celta\", 46, [0, 3, 0, 3, 1]], [\"Elche\", 42, [3, 0, 0, 0, 1]], [\"Espanol\", 42, [1, 1, 0, 1, 0]], [\"Vallecano\", 42, [0, 0, 0, 1, 1]], [\"Cadiz\", 39, [3, 1, 0, 3, 1]], [\"Getafe\", 39, [0, 1, 1, 1, 1]], [\"Mallorca\", 39, [3, 3, 1, 0, 0]], [\"Granada\", 38, [1, 0, 3, 3, 1]], [\"Levante\", 35, [3, 3, 0, 3, 1]], [\"Alaves\", 31, [0, 0, 3, 0, 3]]]\n",
      "Removing temp directory /tmp/laligaMR.root.20230110.102837.425932...\n"
     ]
    }
   ],
   "source": [
    "! python3 laligaMR.py laliga2122.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
