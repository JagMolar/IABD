{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guía práctica de uso de Hive y Pig\n",
    "\n",
    "En la sesión práctica se presentarán los siguientes contenidos:\n",
    "\n",
    "* Presentación del `dataset` que vamos a utilizar.\n",
    "* Conceptos básicos de Hive.\n",
    "* Resolución de consulta compleja con HQL en el `dataset` de vuelos.\n",
    "* Conceptos básicos de Pig.\n",
    "* Resolución de la misma consulta compleja con Pig Latin en el `dataset` de vuelos.\n",
    "  \n",
    "# Dataset de retrasos en vuelos\n",
    "\n",
    "Vamos a usar [este](https://www.kaggle.com/datasets/tylerx/flights-and-airports-data) de Kaggle\n",
    "para aprender a usar tanto Hive como Pig. Kaggle es un sitio muy popular en ciencia de datos. En este sitio los científicos de datos pueden publicar y compartir sus trabajos. Además también se pueden proponer concursos en los que los participantes compiten en la construcción del mejor modelo para el problema propuesto.\n",
    "\n",
    "El `dataset` contiene información sobre retrasos en vuelos en EEUU. Hay dos ficheros de interés: `airports.csv` y `flights.csv`.\n",
    "\n",
    "El primero tiene información sobre los aeropuertos y consta de los siguientes campos:\n",
    "   * airport_id: identificador del aeropuerto. Numérico, aunque se utilizará un campo `string` en Hive.\n",
    "   * city: ciudad del aeropuerto.\n",
    "   * state: estado del aeropuerto.\n",
    "   * name: nombre del aeropuerto.\n",
    "   \n",
    "El fichero `flights` tiene la siguiente estructura:\n",
    "   * DayofMonth: día del mes del vuelo.\n",
    "   * DayOfWeek: día de la semana del vuelo.\n",
    "   * Carrier: Identificador de la compañía aérea.\n",
    "   * OriginAirportID: Identificador del aeropuerto de origen.\n",
    "   * DestAirportID: Identificador del aeropuerto de destino.\n",
    "   * DepDelay: Minutos de retraso en la salida de un vuelo (puede ser negativo si el vuelo sale antes de lo previsto).\n",
    "   * ArrDelay: Minutos de retraso en la llegada de un vuelo (puede ser negativo si el vuelo sale antes de lo previsto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El directorio `notebooks` contiene el `archiv.zip` con los dos ficheros. Para descargarlo de Kaggle hay que estar registrado y se ha incluido para que no tengas que hacerlo. \n",
    "\n",
    "Extraemos los ficheros que nos interesan. El fichero tiene extensión `zip`. Tenemos que instalar el paquete `unzip` ya que no está disponible en el contenedor.\n",
    "\n",
    "Primero tenemos que actualizar los repositorios de paquetes del contenedor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:2 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\u001b[33m\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [991 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1882 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2448 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB][0m\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]    \u001b[0m\u001b[33m\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB][0m\u001b[33m\u001b[33m\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2011 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2924 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1291 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Fetched 25.1 MB in 4s (6158 kB/s)33m                          \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "190 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
     ]
    }
   ],
   "source": [
    "! apt update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego instalamos el paquete `unzip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  zip\n",
      "The following NEW packages will be installed:\n",
      "  unzip\n",
      "0 upgraded, 1 newly installed, 0 to remove and 190 not upgraded.\n",
      "Need to get 168 kB of archives.\n",
      "After this operation, 593 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 unzip amd64 6.0-25ubuntu1.1 [168 kB]\n",
      "Fetched 168 kB in 0s (576 kB/s)0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package unzip.\n",
      "(Reading database ... 43749 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-25ubuntu1.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking unzip (6.0-25ubuntu1.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up unzip (6.0-25ubuntu1.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Processing triggers for mime-support (3.64ubuntu1) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "! apt install unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos los ficheros que nos interesan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip -j -o archive.zip  airports.csv flights.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos el número de líneas y las primeras líneas del fichero de aeropuertos, `airports.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wc -l airports.csv && head airports.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos el número de líneas y las primeras líneas del fichero de aeropuertos, `flights.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wc -l flights.csv && head flights.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, hay 365 aeropuertos (descontada la línea de cabecera) y cerca de tres millones de vuelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiamos los ficheros para hacerlos accesibles en Hadoop. Observa que hemos usado el comando `hdfs` en lugar del comando `hadoop`. Es equivalente hacerlo de una u otra forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir -p /user/root/flights\n",
    "! hdfs dfs -put -f ./airports.csv /user/root/flights/\n",
    "! hdfs dfs -put -f ./flights.csv /user/root/flights/\n",
    "! hdfs dfs -ls /user/root/flights/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive\n",
    "\n",
    "Ya tenemos instalado un servidor de Hive en nuestro `clúster` Hadoop. Hive es probablemente la herramienta más utilizada en el ecosistema Hadoop. La razón es que utiliza un lenguaje de consultas llamado HQL muy similar a SQL.\n",
    "\n",
    "También hay instalado un cliente de Hive llamado `beeline`. Podemos ejecutar comandos de `beeline` en celdas de Jupyter. Por ejemplo, el siguiente comando se conectaría a Hive y mostraría las bases de datos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000\" -e \"SHOW DATABASES\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos crear una nueva base de datos con la siguiente instrucción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/\" -e \"\\\n",
    "CREATE DATABASE IF NOT EXISTS bda03 \\\n",
    "COMMENT 'Base de datos de la unidad BDA03' \\\n",
    "WITH DBPROPERTIES ('Creada por' = 'Javier Pérez', 'Fecha' = '20/12/22');\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso sería crear las tablas para almacenar los datos de los aeropuertos y de los vuelos. En Hive hay dos tipos de tablas:\n",
    "\n",
    "* Internas: Son manejadas completamente por Hive. Hive copiará los datos de los ficheros usados para crear las tablas en el almacenamiento de Hive. Por defecto usará el directorio: `/user/hive/warehouse/database_name.db/`. Cuando se borre la tabla, Hive borrará tanto los datos como los metadatos.\n",
    "* Externas: Los datos no los maneja Hive. Hive únicamente se ocupa de mantener los metadatos. Para crear una tabla externa hay que añadir la opción EXTERNAL. Las tablas que crearemos en este ejercicio son externas.\n",
    "\n",
    "Para mejorar el rendimiento de Hive, las tablas se pueden particionar por el valor de una columna. Hive creará un directorio por cada valor de la columna particionada. La columna de particionamiento realmente no se almacena como un campo, pero en las consultas se mostrará como si realmente existiera ese campo. \n",
    "\n",
    "Por último, hay que tener en cuenta los tipos de datos que soporta Hive. Puedes consultar los tipos soportados [aquí](https://cwiki.apache.org/confluence/display/hive/languagemanual+types).\n",
    "\n",
    "La tabla que almacenará los datos de los aeropuertos se crea así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/bda03\" -e \"\\\n",
    "DROP TABLE IF EXISTS airports; \\\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS airports (airportid STRING, city STRING, state STRING, airportname STRING) \\\n",
    "COMMENT 'USA Airports' \\\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\,' \\\n",
    "TBLPROPERTIES ('Autor' = 'Javier Pérez', 'Fecha' = '20/12/2022', 'skip.header.line.count'='1');\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay varias cuestiones que son interesantes comentar en la anterior instrucción:\n",
    "\n",
    "* En primer lugar, observa que hemos añadido el nombre de la base de datos a la cadena de conexión del cliente `beeline`.\n",
    "* El nombre de la tabla creada se llama `airports`.\n",
    "* La tabla es externa. Eso quiere decir que los datos permanecerán en HDFS y no se moverán al almacenamiento interno de Hive.\n",
    "* La tabla consta de cuatro campos de tipo texto y se corresponden con la descripción que hicimos del fichero `airports.csv`.\n",
    "* Se ha especificado que el delimitador de campos es el carácter coma (,).\n",
    "* Por último, se añade una propiedad que permite eliminar la cabecera del fichero `csv`.\n",
    "\n",
    "La tabla de vuelos es similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/bda03\" -e \"\\\n",
    "DROP TABLE IF EXISTS flights; \\\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS flights (dayofmonth TINYINT, dayofweek TINYINT, carrier STRING, \\\n",
    "    depairportid STRING, arrairportid STRING, depdelay SMALLINT, arrdelay SMALLINT) \\\n",
    "COMMENT 'Flights' \\\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\,' \\\n",
    "TBLPROPERTIES ('Autor' = 'Javier Pérez', 'Fecha' = '20/12/2022', 'skip.header.line.count'='1');\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la tabla `flights` se han ajustado los tipos de datos numéricos para que ocupen lo menos posible. El siguiente paso será cargar los datos. Al tratarse tablas externas, Hive no moverá realmente los datos y será un proceso muy rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de cargar los datos tenemos que dar permisos al directorio de HDFS en el que hemos copiado los ficheros `cvs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -chmod 777 /user/root/flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/bda03\" -e \"\\\n",
    "LOAD DATA INPATH '/user/root/flights/airports.csv' OVERWRITE INTO TABLE airports;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/bda03\" -e \"\\\n",
    "LOAD DATA INPATH '/user/root/flights/flights.csv' OVERWRITE INTO TABLE flights;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si has ejecutado las celdas anteriores, habrás comprobado que el proceso de incorporar datos ha sido muy rápido. Esto es así porque, al tratarse de tablas externas, Hive no necesita copiar los datos y porque Hive no realiza comprobaciones de integridad.\n",
    "\n",
    "Ya podemos hacer consultas. Por ejemplo, la siguiente consulta muestra 10 aeropuertos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/bda03\" -e \"\\\n",
    "SELECT * FROM airports LIMIT 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la siguiente muestra 10 vuelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/bda03\" -e \"\\\n",
    "SELECT * FROM flights LIMIT 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer una consulta para aprender como usar Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulta en Hive: Nombre de los 5 aeropuertos con mayor número de operaciones (llegadas y salidas).\n",
    "\n",
    "Empezamos mostrando las salidas que se producen agrupadas por aeropuerto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/bda03\" -e \"\\\n",
    "SELECT depairportid as airportid, count(*) AS flights FROM flights GROUP BY depairportid \\\n",
    "ORDER BY flights DESC LIMIT 5;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos hacer lo mismo con las llegadas y unir las dos consultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/bda03\" -e \"\\\n",
    "SELECT airportid, COUNT(*) as flights FROM ( \\\n",
    "    SELECT depairportid as airportid FROM flights \\\n",
    "    UNION ALL \\\n",
    "    SELECT arrairportid as airportid FROM flights \\\n",
    ") f GROUP BY airportid \\\n",
    "ORDER BY flights DESC LIMIT 5;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos los códigos de los 5 aeropuertos con más operaciones. Lo único que queda por hacer es obtener el nombre del aeropuerto. Para ello hacemos un `join` con la tabla `airports`. Para hacerlo más inteligible creamos una tabla temporal con los resultados anteriores y el `join` lo hacemos sobre esta tabla temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/bda03\" -e \"\\\n",
    "CREATE TEMPORARY TABLE airport_operations AS \\\n",
    "SELECT airportid, COUNT(*) as flights FROM ( \\\n",
    "    SELECT depairportid as airportid FROM flights \\\n",
    "    UNION ALL \\\n",
    "    SELECT arrairportid as airportid FROM flights \\\n",
    ") f GROUP BY airportid \\\n",
    "ORDER BY flights DESC LIMIT 5; \\\n",
    "\\\n",
    "SELECT airportname, flights \\\n",
    "FROM airport_operations JOIN airports ON airport_operations.airportid = airports.airportid \\\n",
    "ORDER BY flights DESC;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pig\n",
    "\n",
    "Como en el caso de Hive, también tenemos instalado el cliente de Pig en nuestro `cluster` de Hadoop (Pig no tiene servidor). Mientras que Hive es una herramienta pensada para trabajar sobre información estructurada de forma declarativa, Pig puede trabajar sobre información semiestructurada y es una mezcla de programación declarativa y procedimental. Es, por lo tanto, más flexible que Hive. Pig usa un lenguaje de consultas llamado Pig Latin.\n",
    "\n",
    "Al igual que Hive, Pig tiene sus propios tipos de datos. Puedes consultarlos [aquí](https://pig.apache.org/docs/latest/basic.html#data-types).\n",
    "\n",
    "Para ejecutar Pig en Jupyter debemos crear un `script` y ejecutarlo con Pig.\n",
    "\n",
    "Por ejemplo, para leer los ficheros `airports.csv` y `flights.csv` escribimos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile flights.pig\n",
    "\n",
    "-- resgistramos la librería PiggyBank para poder usar la función de carga CSVExcelStorage.\n",
    "REGISTER piggybank.jar\n",
    "\n",
    "/*\n",
    "Leemos el fichero de airports.csv.\n",
    "\n",
    "Usamos el loader CSVExcelStorage indicando el delimitador (,) y que se debe excluir la cabecera.\n",
    "*/\n",
    "\n",
    "AIRPORTS = LOAD '$airports_file' USING\n",
    "       org.apache.pig.piggybank.storage.CSVExcelStorage(',', 'NO_MULTILINE', 'UNIX', 'SKIP_INPUT_HEADER')\n",
    "       AS (airportid:chararray, city:chararray, state:chararray, airportname:chararray);\n",
    "\n",
    "-- Leemos el fichero fligths.csv\n",
    "\n",
    "FLIGHTS = LOAD '$flights_file' USING\n",
    "       org.apache.pig.piggybank.storage.CSVExcelStorage(',', 'NO_MULTILINE', 'UNIX', 'SKIP_INPUT_HEADER')\n",
    "       AS (dayofmonth:int, dayofweek:int, carrier:chararray, \n",
    "               depairportid:chararray, arrairportid:chararray, depdelay:int, arrdelay:int);\n",
    "\n",
    "\n",
    "-- Probamos que podemos recuperar datos.\n",
    "      \n",
    "-- Nos quedamos con 10 aeropuertos\n",
    "AIRPORTS_10 = LIMIT AIRPORTS 10;\n",
    "\n",
    "-- Mostramos 10 aeropuertos\n",
    "DUMP AIRPORTS_10;\n",
    "\n",
    "-- Hacemos lo mismo con los vuelos\n",
    "FLIGHTS_10 = LIMIT FLIGHTS 10;\n",
    "DUMP FLIGHTS_10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pig -x local -f flights.pig -param airports_file='airports.csv' -param flights_file='flights.csv' -param output_dir='pig/output/flights'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa varias cuestiones interesantes:\n",
    "\n",
    "* Los `scripts` de Pig admiten dos tipos de comentarios: orientados a línea y orientados a bloque.\n",
    "* Hemos tenido que registrar la librería `piggybank.jar` para poder usar el `loader` `CSVExcelStorage`. Este `loader` es más potente que el que se usa por defecto en Pig y que se llama `PigStorage`. Concretamente en este ejemplo lo hemos usado para eliminar las líneas de cabecera de los ficheros `csv`.\n",
    "* Cada comando de Pig Latin es atómico (hace una sola operación) y no se pueden componer, con lo que hay que ir haciendo asignaciones sucesivas. Es habitual que las asignaciones se hagan sobre la misma variable sobrescribiéndola. Desde mi punto de vista esa técnica resta claridad y prefiero ir creando nuevas variables según avanza el proceso.\n",
    "* Al ejecutar Pig podemos pasar variables que son accesibles desde el `script`.\n",
    "* He tenido que ejecutar Pig en modo local con la opción `-x local` ya que mi equipo se queda sin memoria si trato de ejecutarlo en Hadoop. Puedes probar a cambiar esta opción y probar si tu equipo soporta la ejecución en el `clúster` de Hadoop.\n",
    "* Observa que la salida del `script` muestra 10 aeropuertos y 10 vuelos con una estructura de datos de tupla. La tupla es uno de los tipos complejos que soporta Pig. Los otros dos tipos complejos son `map` y `bag`. Usaremos el último más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulta en Pig: Nombre de los 5 aeropuertos con mayor número de operaciones (llegadas y salidas).\n",
    "\n",
    "Vamos a resolver la misma consulta que hicimos en Hive pero esta vez utilizando Pig. Seguimos una estrategia parecida a la de Hive: unimos las salidas y las llegadas y agrupamos por aeropuerto. El `script` siguiente está incompleto ya que tan sólo llega hasta hacer la agrupación, pero falta el `join` con aeropuertos para obtener el nombre. Se ha hecho así para explicar que la relación creada con GROUP no tiene a misma estructura que la equivalente con GROUP BY en Hive. Más adelante resolveremos la consulta completamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile flights.pig\n",
    "\n",
    "-- resgistramos la librería PiggyBank para poder usar la función de carga CSVExcelStorage.\n",
    "REGISTER piggybank.jar\n",
    "\n",
    "/*\n",
    "Leemos el fichero de airports.csv.\n",
    "\n",
    "Usamos el loader CSVExcelStorage indicando el delimitador (,) y que se debe excluir la cabecera.\n",
    "*/\n",
    "\n",
    "AIRPORTS = LOAD '$airports_file' USING\n",
    "       org.apache.pig.piggybank.storage.CSVExcelStorage(',', 'NO_MULTILINE', 'UNIX', 'SKIP_INPUT_HEADER')\n",
    "       AS (airportid:chararray, city:chararray, state:chararray, airportname:chararray);\n",
    "\n",
    "-- Leemos el fichero fligths.csv\n",
    "\n",
    "FLIGHTS = LOAD '$flights_file' USING\n",
    "       org.apache.pig.piggybank.storage.CSVExcelStorage(',', 'NO_MULTILINE', 'UNIX', 'SKIP_INPUT_HEADER')\n",
    "       AS (dayofmonth:int, dayofweek:int, carrier:chararray, \n",
    "               depairportid:chararray, arrairportid:chararray, depdelay:int, arrdelay:int);\n",
    "\n",
    "\n",
    "/*\n",
    "    FOREACH ... GENERATE es similar al SELECT de SQL\n",
    "*/\n",
    "DEPARTURES = FOREACH FLIGHTS GENERATE depairportid AS airportid;\n",
    "ARRIVES    = FOREACH FLIGHTS GENERATE arrairportid AS airportid;\n",
    "\n",
    "OPERATIONS = UNION DEPARTURES, ARRIVES;\n",
    "\n",
    "TOTAL_OPERATIONS = GROUP OPERATIONS BY airportid;\n",
    "\n",
    "-- Mostramos el esquema de la relación para que se entienda cómo funciona GROUP\n",
    "DESCRIBE TOTAL_OPERATIONS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pig -x local -f flights.pig -param airports_file='airports.csv' -param flights_file='flights.csv' -param output_dir='pig/output/flights'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa varias cosas:\n",
    "\n",
    "* La consulta ha tardado muy poco tiempo. Esto es debido a que Pig no realiza la consulta hasta que no se muestren los datos por pantalla o se almacenen en un fichero.\n",
    "* La relación TOTAL_OPERATIONS está formada por tuplas con dos campos: `group` y OPERATIONS. El nombre `group` lo ha asignado PIG y contiene el valor del campo por el que hemos agrupado (en este caso el código de aeropuerto). OPERATIONS es un `bag` (lista de tuplas) con las tuplas agrupadas. Es decir, que si mostráramos los datos agrupados, veríamos tuplas con datos similares a estos:\n",
    "    (1, (1,1,1,1,1)), donde 1 sería el código de aeropuerto.\n",
    "    \n",
    "Continuamos con el `script` contando y renombrando campos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile flights.pig\n",
    "\n",
    "-- resgistramos la librería PiggyBank para poder usar la función de carga CSVExcelStorage.\n",
    "REGISTER piggybank.jar\n",
    "\n",
    "/*\n",
    "Leemos el fichero de airports.csv.\n",
    "\n",
    "Usamos el loader CSVExcelStorage indicando el delimitador (,) y que se debe excluir la cabecera.\n",
    "*/\n",
    "\n",
    "AIRPORTS = LOAD '$airports_file' USING\n",
    "       org.apache.pig.piggybank.storage.CSVExcelStorage(',', 'NO_MULTILINE', 'UNIX', 'SKIP_INPUT_HEADER')\n",
    "       AS (airportid:chararray, city:chararray, state:chararray, airportname:chararray);\n",
    "\n",
    "-- Leemos el fichero fligths.csv\n",
    "\n",
    "FLIGHTS = LOAD '$flights_file' USING\n",
    "       org.apache.pig.piggybank.storage.CSVExcelStorage(',', 'NO_MULTILINE', 'UNIX', 'SKIP_INPUT_HEADER')\n",
    "       AS (dayofmonth:int, dayofweek:int, carrier:chararray, \n",
    "               depairportid:chararray, arrairportid:chararray, depdelay:int, arrdelay:int);\n",
    "\n",
    "\n",
    "/*\n",
    "    FOREACH ... GENERATE es similar al SELECT de SQL\n",
    "*/\n",
    "DEPARTURES = FOREACH FLIGHTS GENERATE depairportid AS airportid;\n",
    "ARRIVES    = FOREACH FLIGHTS GENERATE arrairportid AS airportid;\n",
    "\n",
    "OPERATIONS = UNION DEPARTURES, ARRIVES;\n",
    "\n",
    "TOTAL_OPERATIONS = GROUP OPERATIONS BY airportid;\n",
    "\n",
    "-- Mostramos el esquema de la relación para que se entienda cómo funciona GROUP\n",
    "DESCRIBE TOTAL_OPERATIONS;\n",
    "\n",
    "-- Renombramos campos y contamos vuelos\n",
    "TOTAL_OPERATIONS = FOREACH TOTAL_OPERATIONS GENERATE group AS airportid, COUNT(OPERATIONS) AS flights;\n",
    "\n",
    "-- Ordenamos de forma descendente por vuelos\n",
    "TOTAL_OPERATIONS = ORDER TOTAL_OPERATIONS BY flights DESC;\n",
    "\n",
    "-- Limitamos a 5 aeropuertos\n",
    "TOP_TOTAL_OPERATIONS = LIMIT TOTAL_OPERATIONS 5;\n",
    "\n",
    "-- Hacemos un join con la relación de aeropuertos para obtener el nombre\n",
    "TOP_TOTAL_OPERATIONS = JOIN TOP_TOTAL_OPERATIONS BY airportid, AIRPORTS BY airportid;\n",
    "\n",
    "DESCRIBE TOP_TOTAL_OPERATIONS;\n",
    "\n",
    "-- Seleccionamos los campos que nos interesan\n",
    "TOP_TOTAL_OPERATIONS = FOREACH TOP_TOTAL_OPERATIONS GENERATE airportname, flights;\n",
    "\n",
    "-- Volvemos a ordenar por el número de vuelos\n",
    "TOP_TOTAL_OPERATIONS = ORDER TOP_TOTAL_OPERATIONS BY flights DESC;\n",
    "\n",
    "DUMP TOP_TOTAL_OPERATIONS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pig -x local -f flights.pig -param airports_file='airports.csv' -param flights_file='flights.csv' -param output_dir='pig/output/flights'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que en esencia con Pig pordemos hacer lo mismo que con Hive (lo contrario no es siempre cierto), con una sintaxis diferente. Particularmente a mí, en consultas complejas, me parece más fácil entender Pig Latin que HQL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
